A apresentação final do desafio deverá ser realizada por meio de um vídeo, de no máximo, 15 minutos. Esperamos que sua apresentação responda às seguintes perguntas:

-Qual foi o gênero de filmes e séries abordado e qual foi o refinamento que você definiu para sua entrega final?
-Quais foram as etapas do desafio? Como você as desenvolveu? Houveram dificuldades? Nos mostre um pouco do código em execução :)
-Como ficou seu modelo de dados?
-Como ficou seu dashboard? O que os dados estão nos contando?
-Como você imagina que os conhecimentos obtidos no decorrer do Programa de Bolsas podem gerar valor para os clientes da Compass?


"olá, eu sou a emily. faço parte da squad 3 e o nosso tema foi filmes de drama e romance. em cima disso, o tema que eu escolhi abordar no dashboard foi comparação de idade dos atores dos filmes de drama e romance lançados depois de 2010, com intuito de atestar se há etarismo ou não nas escolhas de atores desse genero cinematografico. para chegar nos resultados que eu queria, eu utilizei o arquivo csv movies e dados do elenco dos filmes de drama e romance desse arquivo que eu consegui da api tmdb em forma de json. eu usei do arquivo csv somente as colunas titulooriginal, anolancamento, genero, generoartista, nomeartista, anonascimento e profissao. dos jsons eu usei as colunas name, gender, birthday e known for department. no quicksight eu acabei adicionando mais algumas colunas, que foram idade e intervalosdeidade aos dados vindos do csv e genero e age aos dados vindos do tmdb. 
na primeira etapa do desafio, o objetivo era criar um bucket s3 e fazer o upload dos arquivos movies.csv e series.csv por meio de um código python e fazer ele rodar num conteiner do docker. para fazer isso, eu usei apenas um script e ele ficou assim *mostra o script*. na primeira parte do script, das linhas 3 a 9, é a criaçõ do s3. A segunda parte é o uppload dos arquivos. primeiro eu iniciei a sessão, depois abri os dois arquivos e, por ultimo, mandei os dois para o s3. o dockerfile ficou assim *mostra o dockerfile* e o código para executar o conteiner ficou assim *mostra o código do conteiner*. essa etapa foi um pouco desafiadora pra mim porque eu não tenho muita experiencia com docker, mas fazendo muita pesquisa e com a ajuda dos meus colegas de squad eu consegui chegar a esse resultado. nessa primeira parte é o caminho para as minhas credenciais da aws, na segunda indica pra onde elas vão ser copiadas, para as credenciais do conteiner. depois de tudo pronto, os arquivos ficaram armazenados assim no bucket *mostra a foto de como ficaram*.
na segunda etapa do desafio, o objetivo era fazer a ingestão dos dados do tmdb e persisti-los no s3 na camada raw atraves do aws lambda. o meu código ficou assim *mostra o código*. essa etapa pra mim foi bastante desafiadora, principalmente porque não há muitos dados recentes na internet sobre como fazer as leyers corretamente e também tive problema com o schema de alguns jsons da primeira parte das requisições. para fazer as layers eu fiz muitas tentativas até funcionar, além de precisar pesquisar bastante, foi bem tentativa e erro, e os meus colegas me ajudaram tambem com algumas dicas. na segunda parte eu coloquei essa função pra driblar um erro do lambda. daí, primeiro eu puxei o movies.csv do s3 e transformei num dataframe pandas, daí apliquei alguns   filtros pra conseguir por numa lista apenas os ids dos filmes da minha análise, daí comecei a fazer as primeiras chamadas para conseguir os ids do elenco dos filmes. aqui eu tive o problema com o schema de alguns jsons, porque alguns dos filmes do imdb não tem no tmdb, e quando isso acontecia vinha um json com o schema diferente dos que geravam resultado, aí na hora de gerar outro dataframe dava erro. depois de vários testes, eu resolvi esse problema usando  compreensão de lista pra remover os jsons diferentes. daí desses resultados eu usei somente os dados dos atores, coloquei os ids numa lista e puxei os dados dos atores do tmdb. essa parte demorou mais de 15 minutos então acabou que uma parte dos atores não tiveram seus dados enviados pro s3. pra resolver isso, eu vi no bucket quantos dador tinham sido enviados e reiniciei o teste a partir no id seguinte do ultimo que foi enviado ao s3.
na terceira etapa do desafio, a primeira tarefa era atraves de um job do aws glue usar o spark para padronizar os dados do s3 e enviar para a camada trusted. O meu código ficou assim *mostra o codigo* primeiro eu puxei o movies.csv do s3, fiz a padronização e mandei para o s3 em parquet. depois fiz o mesmo com o series.csv e com os jsons do tmdb. nessa etapa eu tive problema em usar o gluecontext para mandar os dados em parquet pro s3. mesmo pesquisando muito eu não consegui resolver o problema, mas conversando sobre isso com os meus colegas, percebi que dava para fazer isso sem usar o glue context, mas usando o spark, e inclusive até pra puxar os dados do s3 funciona com o spark e é muito mais simples. eu acho que ajudaria muito se nessa fase tivesse um material que deixasse claro pra gente que não precisa usar gluecontext pra puxar ou enviar os dados do s3, fica aí a sugestão. as tarefas 2 e 3 dessa etapa do desafio eu tratei como se fossem uma só, porque pra mim, pareceu que elas pediam a mesma coisa. primeiro, eu puxei o parquet que era o movies.csv da trusted, fiz modificações do tipo de dados de duas colunas que estavam como string, e eu precisava que estivessem em inteiro, daí mandei para a camada refined. depois puxei os dados do tmdb da trusted, filtrei e tambem fiz mudança de tipo de dados de uma coluna. daí mandei pra camada refined. agora eu vou rodar esse codigo pra depois ver o resultado no s3. depois disso, eu fiz dois crawlers pra enviar esses dados para o data catalog, pras tabelas aparecerem no athena e eu conseguir usar no quicksight, esses são os crawlers *mostra os crawlers*. muita coisa que eu fiz nesse código eu fiz depois que eu comecei a fazer o dashboard, como por exemplo o filtro na tabela gender. o tmdb usa 1 para feminino e 2 para masculino, mas quando eu comecei a fazer os graficos, percebi que, na verdade, eles usam 0, 1, 2, 3 e 4, sendo que 0, 3 e 4 são os dois generos misturados, então eu precisei filtrar pra ficar só os dados confiáveis. 
Na ultima etapa do desafio, na verdade eu fiz dois dashboards, um com dados do movies.csv e outro com dados do tmdb. apesar da segunda tabela complementar a primeira, não tinha uma chave comum entre elas pra fazer join, e tambem eu achei que ficou mais interessante analisar as duas separadas, então eu optei por fazer dois dashboards. eu acabei adicionando algumas colunas nas tabelas pelo quicksight pra complementar a minha análise. a primeira coluna adicionada foi a coluna idade, ela contem a informação de que idade os atores tinham quando particiaram dos filmes, foi conseguida pela subtração entre as colunas anolancamento e anonascimento. a segunda coluna adicionada foi intervalosdeidade, é mais uma organização da tabela idade. o primeiro dashboard ficou assim. o primeiro gráfico é a comparação por intervalo de idade de quantidade de atores. percebe-se que há quase o dobro de atrizes na casa dos 20 anos comparado ao numero de atores nessa faixa etária, e quanto mais vai aumentando o intervalo, não só a quantidade geral de atores cai drasticamente, mas também o número de atores mais velhos é superior ao numero de atrizes mais velhas. o segundo gráfico é a media das idades dos atores, que deixa  claro que no geral, as atrizes são mais novas que os atores, sendo aproximadamente 38 anos para os homens e 33 anos e meio para as mulheres. o terceiro é uma tabela que demonstra a diferença de idade dos atores por filme. as cores claras indicam menores idades e as escuras indicam maiores idades. é interessante perceber que em vários dos filmes, as atrizes são mais novas que os atores. com essa análise, percebe-se que há sinais de etarismo tanto na escolha dos atores quanto das atrizes, pois a média das idades é baixa. além disso, as mulheres são mais atingidas, pois além da maioria ter entre 20 e 40 anos, a média é consideravelmente mais baixa que a dos homens.
no segundo dashboard foram usados os dados do tmdb. a primeira tabela adicionada foi age, que contem as idades dos atores. o calculo foi feito com a subtração entre 2023 e o ano de nascimento dos atores. a segunda coluna adicionada foi genero, que contem a 'tradução' da tabela gender, pra ficar mais claro qual é qual nos gráficos. no primeiro gráfico tem a média de idade por genero, e mostra que as atrizes são mais novas que os atores no geral. o segundo mostra a distribuição dos dados de idade, sendo que a idade máxima dos atores é 92, 75% dos atores tem no máximo 59 anos, a idade mediana é 47, 25% dos atores tem no máximo 37 anos e a idade mínima é 7. quanto as atrizes, a idade máxima é 80, 75% das atrizes tem no maximo 52 anos, a idade mediana é 40, 25% das atrizes tem no máximo 33 anos e a idade mínima é 16. o terceiro gráfico mostra a média geral das idades, que é aproximadamente 46 anos. com essa análise entende-se que há sinais brandos de etarismo na escolha dos atores e as mulheres são mais atingidas.
com certeza a proposta do programa de bolsas tem um potencial muito grande de gerar valor para os clientes da compass, porque os serviços da aws facilitam o trabalho no geral, pois todas as ferramentas ficam juntas na mesma plataforma e apesar de, no começo, varias serem complicadas de entender, com prática o trabalho flui e é possível conseguir insights muito interessantes e até decisivos pros clientes, dependendo da solução com a qual eles trabalham. os conhecimentos sobre docker, spark, sql e das ferramentas da aws se usados em conjunto pra fazer analise de grandes conjuntos de dados e assim conseguir insights estratégicos com certeza é o caminho pro sucesso de uma empresa nos dias de hoje eu acredito que no futuro, isso vai ser crucial pra uma empresa existir e se destacar entre os concorrentes.
por fim, o job já finalizou com sucesso e esses são os resultados no s3."
